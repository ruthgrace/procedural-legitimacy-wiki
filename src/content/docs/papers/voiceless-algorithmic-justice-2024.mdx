---
title: "'Voiceless': The Procedural Gap in Algorithmic Justice — Kinchin (2024)"
description: "'Voiceless': The Procedural Gap in Algorithmic Justice"
---

**Authors:** Niamh Kinchin

**Published:** 2024, *International Journal of Law and Information Technology*, Vol. 32, No. 1, eaae024

**Domain:** Algorithmic decision-making, procedural justice, and legal accountability

When an algorithm denies someone a loan, flags them for a welfare fraud investigation, or determines their risk score in a criminal proceeding, the affected individual typically has no opportunity to provide input before the decision is made. Kinchin identifies this as a "procedural gap" in algorithmic justice — a systematic deficit of "voice" that pervades algorithmic decision-making across legal and institutional contexts. While much of the scholarship on algorithmic fairness has focused on distributional concerns (whether algorithms produce biased outcomes across racial, gender, or socioeconomic groups), Kinchin argues that this emphasis has come at the expense of procedural justice. The result is that affected individuals are rendered "voiceless" — they cannot provide information, contest reasoning, or be heard before a decision is finalized.

The paper traces this procedural gap across four domains: technical fairness metrics, AI ethics guidelines, formal legal regulation, and judicial review. In each domain, Kinchin finds that distributional justice is better served than procedural justice. Fairness metrics are designed to detect outcome disparities, not to provide process safeguards. AI ethics guidelines invoke fairness in vague terms that rarely translate into enforceable procedural rights. Regulatory frameworks like the EU AI Act address risk classification and transparency but stop short of guaranteeing meaningful voice. And judicial review, as examined across Australian common law, U.S. due process, and European subsidiarity principles, faces jurisdiction-specific barriers that limit its ability to assure voice in algorithmic contexts. The paper concludes that algorithmic rights must include the right to provide information before a decision is finalized — a principle that mirrors the "voice" concept in Tyler's procedural justice framework but requires new institutional mechanisms to operationalize in algorithmic settings.

## Background

Kinchin's analysis draws on the procedural justice tradition originating with Thibaut and Walker (1975) and developed by Tyler (1990, 2006), which holds that people evaluate the legitimacy of decisions based substantially on whether they had voice in the process — not just whether the outcome was favorable. The paper applies this framework comparatively across three legal systems (Australia, the United States, and the European Union) to assess whether existing accountability structures can deliver procedural justice for algorithmic decisions. The comparative legal method reveals that no jurisdiction currently provides robust protections for voice in algorithmic governance.

## Key Arguments

| Domain | Distributional Justice Coverage | Procedural Justice (Voice) Coverage |
|---|---|---|
| Technical fairness metrics | Well-served: bias detection and statistical parity | Poorly served: no mechanism for individual input |
| AI ethics guidelines | Mentioned but vague | Rarely translated into enforceable procedural rights |
| Legal regulation (e.g., EU AI Act) | Risk-based classification addresses some harms | No guarantee of voice before automated decisions |
| Judicial review (Australia) | Common law review can assess outcome fairness | Procedural fairness doctrines struggle with algorithmic opacity |
| Judicial review (US) | Equal protection addresses disparate impact | Due process clause limited in application to algorithmic decisions |
| Judicial review (EU) | Subsidiarity and GDPR provide some protections | Right to explanation falls short of meaningful voice |

## Open Questions

- What concrete institutional mechanisms could guarantee "voice" in algorithmic decision-making — for example, mandatory pre-decision notice and comment periods for high-stakes automated decisions?
- How should the right to voice be balanced against the efficiency gains that motivate algorithmic governance in the first place?
- Can the procedural gap be addressed through participatory algorithm design (upstream voice), or does it require individual contestation rights (downstream voice), or both?

## Further Reading

- Tyler, T. R. (2006). *Why People Obey the Law*. Princeton University Press.
- Thibaut, J., & Walker, L. (1975). *Procedural Justice: A Psychological Analysis*. Erlbaum.
- Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019). "Fairness and abstraction in sociotechnical systems." *Proceedings of the Conference on Fairness, Accountability, and Transparency*, 59–68.
